networks:
  vllm_network:
    driver: bridge

services:
  gateway:
    build:
      context: ./gateway
      dockerfile: Dockerfile
    image: my-vllm-gateway:latest
    container_name: vllm_gateway
    ports:
      - "9003:9000"
    environment:
      # --- REQUIRED ---
      HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN}
      VLLM_IMAGE: ${VLLM_IMAGE:-vllm/vllm-openai:v0.10.2}
      VLLM_GPU_MEMORY_UTILIZATION: ${VLLM_GPU_MEMORY_UTILIZATION:-0.90}
      VLLM_SWAP_SPACE: ${VLLM_SWAP_SPACE:-4}
      VLLM_MAX_MODEL_LEN_GLOBAL: ${VLLM_MAX_MODEL_LEN_GLOBAL:-40000}
      VLLM_MAX_NUM_SEQS: ${VLLM_MAX_NUM_SEQS:-16}
      VLLM_TENSOR_PARALLEL_SIZE: ${VLLM_TENSOR_PARALLEL_SIZE:-1}
      VLLM_PORT: ${VLLM_PORT:-8000}
      LOG_LEVEL: ${LOG_LEVEL:-DEBUG}

      # --- NETWORKING ---
      DOCKER_NETWORK_NAME: ${DOCKER_NETWORK_NAME:-vllm_network}
      GATEWAY_CONTAINER_NAME: ${GATEWAY_CONTAINER_NAME:-vllm_gateway}
      VLLM_INACTIVITY_TIMEOUT: ${VLLM_INACTIVITY_TIMEOUT:-7200}
      VLLM_CONTAINER_PREFIX: ${VLLM_CONTAINER_PREFIX:-vllm_server}

      # --- PATHS ---
      HOST_CACHE_DIR: ${HOST_CACHE_DIR:-/root/.cache/huggingface}
      MEMORY_FOOTPRINT_FILE: ${MEMORY_FOOTPRINT_FILE:-/app/memory_footprints.json}
      VLLM_TEMP_DIR: ${VLLM_TEMP_DIR:-/tmp}

      # --- DOCKER IMAGES ---
      NVIDIA_UTILITY_IMAGE: ${NVIDIA_UTILITY_IMAGE:-nvidia/cuda:12.1.0-base-ubuntu22.04}

      # --- CONFIGURABLE ALLOWED MODELS ---
      # Override with your own JSON in .env file or Portainer
      ALLOWED_MODELS_JSON: ${ALLOWED_MODELS_JSON:-{"gemma3-4B":"google/gemma-3-4b-it","gemma3-12b-awq":"gaunernst/gemma-3-12b-it-qat-autoawq","gemma3-27B":"gaunernst/gemma-3-27b-it-int4-awq","gemma3-27b-4bit":"unsloth/gemma-3-27b-it-qat-unsloth-bnb-4bit","gpt-oss-20b":"openai/gpt-oss-20b","qwen2.5-coder":"Qwen/Qwen2.5-Coder-7B-Instruct","qwen3-8B":"Qwen/Qwen3-8B","deepseek-r1-7B":"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B","deepseek-r1-32B":"Valdemardi/DeepSeek-R1-Distill-Qwen-32B-AWQ","llama3-8B":"meta-llama/Meta-Llama-3-8B-Instruct","bge-large-en":"BAAI/bge-large-en-v1.5","bge-reranker-v2-m3":"BAAI/bge-reranker-v2-m3"}}

    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ${HOST_CACHE_DIR:-/root/.cache/huggingface}:/root/.cache/huggingface
      - ${HOST_MEMORY_FOOTPRINT_FILE:-./memory_footprints.json}:${MEMORY_FOOTPRINT_FILE:-/app/memory_footprints.json}
      - ${HOST_TEMP_DIR:-/tmp}:${VLLM_TEMP_DIR:-/tmp}

    networks:
      - vllm_network

    restart: unless-stopped
