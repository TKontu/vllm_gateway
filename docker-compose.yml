networks:
  vllm_network:
    driver: bridge

services:
  gateway:
    build:
      context: ./gateway
      dockerfile: Dockerfile
    image: my-vllm-gateway:latest
    container_name: vllm_gateway
    ports:
      - "9003:9000"
    environment:
      # --- REQUIRED ---
      HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN}
      VLLM_IMAGE: ${VLLM_IMAGE:-vllm/vllm-openai:v0.10.2}
      VLLM_GPU_MEMORY_UTILIZATION: ${VLLM_GPU_MEMORY_UTILIZATION:-0.90}
      VLLM_SWAP_SPACE: "4"
      VLLM_MAX_MODEL_LEN_GLOBAL: "40000"
      VLLM_MAX_NUM_SEQS: ${VLLM_MAX_NUM_SEQS:-16}
      VLLM_TENSOR_PARALLEL_SIZE: ${VLLM_TENSOR_PARALLEL_SIZE:-1}
      LOG_LEVEL: "DEBUG"

      # --- NETWORKING ---
      DOCKER_NETWORK_NAME: vllm_network
      GATEWAY_CONTAINER_NAME: "vllm_gateway"
      VLLM_INACTIVITY_TIMEOUT: "7200"

      # --- PATHS ---
      HOST_CACHE_DIR: "/root/.cache/huggingface"

      # --- CONFIGURABLE ALLOWED MODELS ---
      ALLOWED_MODELS_JSON: |
        {
          "gemma3-4B": "google/gemma-3-4b-it",
          "gemma3-12b-awq": "gaunernst/gemma-3-12b-it-qat-autoawq",
          "gemma3-27B": "gaunernst/gemma-3-27b-it-int4-awq",
          "gemma3-27b-4bit": "unsloth/gemma-3-27b-it-qat-unsloth-bnb-4bit",
          "gpt-oss-20b": "openai/gpt-oss-20b",
          "qwen2.5-coder": "Qwen/Qwen2.5-Coder-7B-Instruct",
          "qwen3-8B": "Qwen/Qwen3-8B",
          "deepseek-r1-7B": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "deepseek-r1-32B": "Valdemardi/DeepSeek-R1-Distill-Qwen-32B-AWQ",
          "llama3-8B": "meta-llama/Meta-Llama-3-8B-Instruct",
          "bge-large-en": "BAAI/bge-large-en-v1.5"
        }

    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /root/.cache/huggingface:/root/.cache/huggingface
      - ./memory_footprints.json:/app/memory_footprints.json

    networks:
      - vllm_network

    restart: unless-stopped
