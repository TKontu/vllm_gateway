# ========================================
# vLLM Gateway Configuration
# ========================================
# Copy this file to .env and customize the values for your deployment
# All variables have sensible defaults - only set what you need to override

# ========================================
# REQUIRED
# ========================================

# HuggingFace Hub token for accessing gated models
# Get your token from: https://huggingface.co/settings/tokens
HUGGING_FACE_HUB_TOKEN=

# ========================================
# VLLM ENGINE SETTINGS
# ========================================

# vLLM Docker image to use for model containers
# Default: vllm/vllm-openai:v0.10.2
VLLM_IMAGE=vllm/vllm-openai:v0.10.2

# GPU memory utilization (0.0 to 1.0)
# Default: 0.90 (90% of available GPU memory)
VLLM_GPU_MEMORY_UTILIZATION=0.90

# CPU swap space in GB (for offloading when GPU memory is full)
# Default: 16
VLLM_SWAP_SPACE=16

# Global maximum model length (context window)
# Set to 0 to use model's default. Otherwise limits all models to this value.
# Default: 0
VLLM_MAX_MODEL_LEN_GLOBAL=0

# Maximum number of concurrent sequences
# Default: 16
VLLM_MAX_NUM_SEQS=16

# Tensor parallel size (number of GPUs to split model across)
# Default: 1
VLLM_TENSOR_PARALLEL_SIZE=1

# Internal port used by vLLM containers
# Default: 8000
VLLM_PORT=8000

# ========================================
# NETWORKING
# ========================================

# Docker network name for vLLM containers
# Default: vllm_network
DOCKER_NETWORK_NAME=vllm_network

# Gateway container name (must match docker-compose.yml)
# Default: vllm_gateway
GATEWAY_CONTAINER_NAME=vllm_gateway

# Container name prefix for vLLM model servers
# Default: vllm_server
VLLM_CONTAINER_PREFIX=vllm_server

# Inactivity timeout in seconds (containers shut down after this period of no requests)
# Set to 0 to disable automatic shutdown
# Default: 1800 (30 minutes)
VLLM_INACTIVITY_TIMEOUT=1800

# ========================================
# QUEUE MANAGEMENT
# ========================================

# Maximum number of requests that can wait in queue per model
# Requests exceeding this limit will receive HTTP 429 (Too Many Requests)
# Default: 200
GATEWAY_MAX_QUEUE_SIZE=200

# Maximum number of concurrent requests forwarded to vLLM per model
# This prevents overwhelming vLLM and ensures no requests are lost
# Adjust based on your vLLM configuration (typically 2-4x VLLM_MAX_NUM_SEQS)
# Default: 50
GATEWAY_MAX_CONCURRENT=50

# ========================================
# PATHS (HOST MACHINE)
# ========================================

# Cache directory for HuggingFace downloads (on host machine)
# This is mounted into containers for sharing model downloads
# Default: /root/.cache/huggingface
HOST_CACHE_DIR=/root/.cache/huggingface

# Data directory for persistent files (on host machine)
# The application will create files inside this directory
# No need to pre-create files - just ensure the directory exists
# Default: ./data
HOST_DATA_DIR=./data

# Temporary directory for large file downloads (on host machine)
# Default: /tmp
HOST_TEMP_DIR=/tmp

# ========================================
# PATHS (CONTAINER)
# ========================================

# Memory footprint storage file path inside the gateway container
# The app will create this file automatically inside the mounted data directory
# Default: /app/data/memory_footprints.json
MEMORY_FOOTPRINT_FILE=/app/data/memory_footprints.json

# Temporary directory inside vLLM containers (for GGUF downloads)
# Default: /tmp
VLLM_TEMP_DIR=/tmp

# ========================================
# DOCKER IMAGES
# ========================================

# NVIDIA utility container image (used for nvidia-smi queries)
# Default: nvidia/cuda:12.1.0-base-ubuntu22.04
NVIDIA_UTILITY_IMAGE=nvidia/cuda:12.1.0-base-ubuntu22.04

# ========================================
# LOGGING
# ========================================

# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
# Default: INFO
LOG_LEVEL=INFO

# ========================================
# ALLOWED MODELS
# ========================================

# JSON mapping of user-friendly model names to HuggingFace model IDs
# Format: {"user-name": "org/repo-id"}
#
# The docker-compose.yml includes a default list of models.
# Override here or in Portainer to use your own model list.
#
# Example with multiple models (single line JSON):
# ALLOWED_MODELS_JSON={"gemma3-4B":"google/gemma-3-4b-it","llama3-8B":"meta-llama/Meta-Llama-3-8B-Instruct","qwen2.5":"Qwen/Qwen2.5-Coder-7B-Instruct"}

# For Portainer, you can use multi-line format in the web UI:
# {
#   "gemma3-4B": "google/gemma-3-4b-it",
#   "llama3-8B": "meta-llama/Meta-Llama-3-8B-Instruct",
#   "qwen2.5": "Qwen/Qwen2.5-Coder-7B-Instruct"
# }
